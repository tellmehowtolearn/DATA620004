# DATA620004
homework of DATA620004

## 1. 参数查找：学习率，隐藏层大小，正则化强度
三个参数分别有三种选择：学习率有[1, 5, 0.5]三种选择，隐藏层大小有[100, 150, 200]三种选择，正则化强度有[1e-5, 1e-4, 1e-3]三种选择，所以总共的组合有27种，选择在同等训练次数下最终达到的loss最小的组合作为最终的参数组合。（训练次数在当前文件中定义为200次，对应的文件为search_parm.py）

## 2. 训练

构建的类定义在search_parm.py中

### 2.1. 激活函数

在此次模型构建中，激活函数采用sigmoid函数，即$$h(x)=\frac{1}{1+e^{-x}}$$.

### 2.2. 反向传播，loss以及梯度的计算

loss函数采用交叉熵函数加上L2正则化的形式，定义在类中的loss_function函数。

关于梯度的计算，依次根据公式Downstream Gradient = Local Gradient*Upstream Gradient反向求解，最终求得loss function关于权重矩阵$$w_1$$和$$w_2$$的偏导。特别地，先不管loss function中的正则化项，L(softmax(y_pred_pre))关于y_pred_pre的偏导就是softmax(y_pred_pre)-y_truth的值，因此这里可以直接跳过一步求导，其他的依次求导回去求导，最后再加上正则化项关于$$w_1$$和$$w_2$$的偏导，定义在类中的gradient_compute函数。

### 2.3. 学习率下降策略

采取固定迭代次数学习率衰减的策略，具体实现在类中的train函数，下降率为decrease_gamma变量，这里设为0.99，每隔100次迭代下降一次

### 2.4. L2正则化

正则化强度定义为nu，正则化项在损失函数中定义为$$\frac12\Vert w_1 \Vert_2^2+\frac12\Vert w_2 \Vert_2^2 $$。最终关于$w_1$的偏导也要加上$$w_1$$

，同样地关于$w_2$的偏导也要加上$$w_2$$.

### 2.5.  优化器SGD

即随机选取几个数据进行梯度的计算和反向传播，这里定义选取sample_size大小的数据，同样实现在类中的gradient_compute函数。

### 2.6. 保存模型

保存训练好的$$w_1$$和$$w_2$$矩阵权重大小，用numpy的savetxt函数进行保存，具体实现在类中的store_values函数。

#### 最终模型训练

利用参数查找得到的参数进行最后模型的训练，定义在model_train.py文件中，并保存好训练的权重矩阵。



## 测试

导入模型进行测试，输出分类精度，对应的文件为load_model.py





### 训练和测试步骤

训练步骤：先前向计算出预测值并计算好loss函数，再反向求出关于$$w_1$$和$$w_2$$的偏导，根据SGD优化策略更新权重矩阵，不断迭代。

测试步骤：前向计算出的预测值根据概率最大的那项预测分类结果。

文件执行顺序：

先执行search_parm.py文件，进行参数的查找并会自动将参数保存好放在子文件夹data当中，再执行model_train.py文件，会利用查找好的参数进行模型的训练，最终模型的权重矩阵自动保存放在子文件夹data当中，最后执行load_model.py文件，会利用查找好的参数及权重矩阵数值进行模型的重构并对测试集进行测试。

